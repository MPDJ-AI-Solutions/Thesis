{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Environment setup",
   "id": "9ad61cf8537246a"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-12T12:43:12.837674Z",
     "start_time": "2024-12-12T12:43:12.833835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Optional\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from dataset.dataset_info import ClassifierDatasetInfo\n",
    "from dataset.dataset_type import DatasetType\n",
    "from dataset.STARCOP_dataset import STARCOPDataset\n",
    "\n",
    "from models.Tools.FilesHandler.model_files_handler import ModelFilesHandler\n",
    "from models.Tools.Measures.measure_tool_factory import MeasureToolFactory\n",
    "from models.Tools.Measures.model_type import ModelType\n",
    "\n",
    "import os\n",
    "os.chdir(r\"D:\\Projects\\studia\\polsl_ssi_1\\MethaneDetection\\Thesis\")"
   ],
   "id": "6d686f408b02c2f8",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup datasets\n",
    "STARCOPDataset is custom class that derives torch.utils.data.Dataset class. It's defined in dataset module."
   ],
   "id": "e84bf51a990e7e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T12:43:12.843289Z",
     "start_time": "2024-12-12T12:43:12.840190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def setup_dataloaders(data_path: str = r\"data\", batch_size: int = 32, train_type = DatasetType.EASY_TRAIN):\n",
    "    train_dataset = STARCOPDataset(\n",
    "        data_path=data_path,\n",
    "        data_type=train_type,\n",
    "        image_info_class=ClassifierDatasetInfo,\n",
    "    )\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = STARCOPDataset(\n",
    "        data_path=data_path,\n",
    "        data_type=DatasetType.TEST,\n",
    "        image_info_class=ClassifierDatasetInfo,\n",
    "    )\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ],
   "id": "c3d8add62bc5a28b",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup models\n",
    "\n",
    "### Model class"
   ],
   "id": "37801b5aea4f5b0d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T12:43:12.862049Z",
     "start_time": "2024-12-12T12:43:12.858135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MethaNetClassifier(nn.Module):\n",
    "    def __init__(self, in_channels:int = 9, num_classes:int = 2):\n",
    "        super(MethaNetClassifier, self).__init__()\n",
    "        self.pre_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels//1, in_channels//2, kernel_size=1),\n",
    "            nn.Conv2d(in_channels//2, in_channels//4, kernel_size=1),\n",
    "            nn.Conv2d(in_channels//4, in_channels//8, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=2, stride=1, padding=0),  # Input channels = 8\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(6, 12, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(12, 16, kernel_size=4, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Dropout(p=0.2),\n",
    "\n",
    "            nn.Conv2d(16, 16, kernel_size=4, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 61 * 61, 64),  # Adjust dimensions for 512x512 input\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(32, num_classes),\n",
    "            nn.Softmax(dim=1)  # Softmax for class probabilities\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_conv(x)\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n"
   ],
   "id": "388d223200856cfb",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prepare models",
   "id": "9079395251cbc5b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T12:43:12.867598Z",
     "start_time": "2024-12-12T12:43:12.864053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def setup_model(model: nn.Module, lr: float, device: str):\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()  # Binary classification\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    return model, criterion, optimizer"
   ],
   "id": "c6f167b1b416710a",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prepare training function",
   "id": "8b87ecfebbda635c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T12:43:12.878640Z",
     "start_time": "2024-12-12T12:43:12.874911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_cnn(criterion, device, epochs, model, optimizer, dataloader, transform: Optional[transforms] = None, log_batches: bool = False):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):  # Adjust the number of epochs\n",
    "        running_loss = 0.0\n",
    "        for batch_id, (images, mag1c, labels) in enumerate(dataloader):  # Assume a PyTorch DataLoader is set up\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_image = torch.cat((images, mag1c), dim=1)\n",
    "            labels = labels.long().to(device)\n",
    "\n",
    "            outputs = model((transform(input_image) if transform else  input_image).to(device))\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(dataloader)}\")"
   ],
   "id": "512fceef38a11323",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prepare evaluate function",
   "id": "3f530c417b88fc19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T12:43:12.889167Z",
     "start_time": "2024-12-12T12:43:12.886162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_cnn(criterion, device, model, dataloader, measurer, transform: Optional[transforms] = None):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_id, (images, mag1c, labels) in enumerate(dataloader):\n",
    "        input_image = torch.cat((images, mag1c), dim=1)\n",
    "        labels = labels.long().to(device)\n",
    "\n",
    "        outputs = model((transform(input_image) if transform else  input_image).to(device))\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        all_predictions.append(predictions.cpu().detach())\n",
    "        all_labels.append(labels.cpu().detach())\n",
    "\n",
    "    measures = measurer.compute_measures(torch.cat(all_predictions), torch.cat(all_labels))\n",
    "    print(f\"Validation loss: {running_loss / len(dataloader)}.\\nMeasures:\\n{measures}\")\n",
    "    return measures"
   ],
   "id": "8cf2bd45a43c4576",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train model",
   "id": "545bc7a7fe0620d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T12:47:08.937328Z",
     "start_time": "2024-12-12T12:43:12.896180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 10\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 1e-5\n",
    "\n",
    "train_dataloader, test_dataloader = setup_dataloaders(batch_size=16)\n",
    "model = MethaNetClassifier()\n",
    "model, criterion, optimizer = setup_model(model, lr, device)\n",
    "\n",
    "model_handler = ModelFilesHandler()\n",
    "measurer = MeasureToolFactory.get_measure_tool(ModelType.CNN)\n",
    "\n",
    "train_cnn(criterion, device, epochs, model, optimizer, train_dataloader, log_batches=True)\n",
    "measures = evaluate_cnn(criterion, device, model, test_dataloader, measurer)"
   ],
   "id": "63a2f27507c95583",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6540219289915902\n",
      "Epoch 2, Loss: 0.555686570916857\n",
      "Epoch 3, Loss: 0.48661099672317504\n",
      "Epoch 4, Loss: 0.4395524263381958\n",
      "Epoch 5, Loss: 0.408256835596902\n",
      "Epoch 6, Loss: 0.3862720421382359\n",
      "Epoch 7, Loss: 0.36930905069623676\n",
      "Epoch 8, Loss: 0.3594409908567156\n",
      "Epoch 9, Loss: 0.34937363607542854\n",
      "Epoch 10, Loss: 0.3438535213470459\n",
      "Validation loss: 0.5237618982791901.\n",
      "Measures:\n",
      "         TP        FP        FN        TN  Precision  Sensitivity  \\\n",
      "0  0.263158  0.002924  0.225146  0.508772   0.989011     0.538922   \n",
      "\n",
      "   Specificity       NPV       FPR        Accuracy   F-Score             IoU  \\\n",
      "0     0.994286  0.693227  0.005714  tensor(0.7719)  0.697674  tensor(0.5357)   \n",
      "\n",
      "        MCC       AUC          CI  \n",
      "0  0.603137  0.766604  (1.0, 1.0)  \n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save model",
   "id": "c8a7c96c4906c574"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T12:47:08.977638Z",
     "start_time": "2024-12-12T12:47:08.952330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_handler.save_model(\n",
    "    model=model,\n",
    "    metrics = measures,\n",
    "    model_type=ModelType.CNN,\n",
    "    epoch=epochs,\n",
    ")"
   ],
   "id": "90703884c752a84",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trained_models\\\\model_cnn_2024_12_12_13_47_08.pickle'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
