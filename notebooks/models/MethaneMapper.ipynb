{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db158e146b6a2ab0",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This is the training showcase of Methane Mapper model. Can be used to train new models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2788c92659ad2f",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T00:39:31.742659Z",
     "start_time": "2024-12-17T00:39:25.193127Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir(r\"..\\..\")\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset.dataset_info import ClassifierDatasetInfo, MMClassifierDatasetInfo\n",
    "from dataset.dataset_type import DatasetType\n",
    "from dataset.STARCOP_dataset import STARCOPDataset\n",
    "\n",
    "from files_handler import ModelFilesHandler\n",
    "from measures import MeasureToolFactory\n",
    "from measures import ModelType\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1080e782a0be239",
   "metadata": {},
   "source": [
    "## Setup datasets\n",
    "STARCOPDataset is custom class that derives torch.utils.data.Dataset class. It's defined in dataset module.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "633a22ebce25443b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T00:39:31.752969Z",
     "start_time": "2024-12-17T00:39:31.748769Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "from dataset.dataset_info import DatasetInfo\n",
    "\n",
    "\n",
    "def setup_dataloaders(\n",
    "        data_path: str = r\"data\",\n",
    "        batch_size: int = 32,\n",
    "        train_type = DatasetType.EASY_TRAIN,\n",
    "        image_info_class: Type[DatasetInfo] = ClassifierDatasetInfo,\n",
    "        crop_size: int = 1\n",
    "):\n",
    "    train_dataset = STARCOPDataset(\n",
    "        data_path=data_path,\n",
    "        data_type=train_type,\n",
    "        image_info_class=image_info_class,\n",
    "        crop_size=crop_size\n",
    "    )\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = STARCOPDataset(\n",
    "        data_path=data_path,\n",
    "        data_type=DatasetType.TEST,\n",
    "        image_info_class=image_info_class,\n",
    "        crop_size=crop_size\n",
    "    )\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size // 2, shuffle=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aade7bf5249f30",
   "metadata": {},
   "source": [
    "## Setup models\n",
    "\n",
    "### Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f28a27507f2ad99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T00:39:31.855394Z",
     "start_time": "2024-12-17T00:39:31.844945Z"
    }
   },
   "outputs": [],
   "source": [
    "from models.Transformer.MethaneMapper.Segmentation.bbox_prediction import BBoxPrediction\n",
    "from models.Transformer.MethaneMapper.Segmentation.segmentation import BoxAndMaskPredictor\n",
    "from models.Transformer.MethaneMapper.Classification.classification import ClassifierPredictor\n",
    "from models.Transformer.MethaneMapper.Transformer.hyperspectral_decoder import HyperspectralDecoder\n",
    "from models.Transformer.MethaneMapper.Transformer.query_refiner import QueryRefiner\n",
    "from models.Transformer.MethaneMapper.Transformer.encoder import Encoder\n",
    "from models.Transformer.MethaneMapper.Transformer.position_encoding import PositionalEncodingMM\n",
    "from models.Transformer.MethaneMapper.SpectralFeatureGenerator.spectral_feature_generator import \\\n",
    "    SpectralFeatureGenerator\n",
    "from models.Transformer.MethaneMapper.Backbone.backbone import Backbone\n",
    "from models.Transformer.MethaneMapper.model_type import ModelType\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    TODO docs, verification, tests\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model: int = 256,\n",
    "                 backbone_out_channels: int = 2048,\n",
    "                 image_height: int = 512,\n",
    "                 image_width: int = 512,\n",
    "                 attention_heads: int = 8,\n",
    "                 n_encoder_layers: int = 6,\n",
    "                 n_decoder_layers: int = 6,\n",
    "                 n_queries: int = 100,\n",
    "                 threshold: float = 0.5,\n",
    "                 model_type: ModelType = ModelType.CLASSIFICATION,\n",
    "                 ):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.backbone = Backbone(d_model=d_model, rgb_channels=3, swir_channels=5, out_channels=backbone_out_channels)\n",
    "        self.spectral_feature_generator = SpectralFeatureGenerator(d_model=d_model)\n",
    "\n",
    "        self.positional_encoding = PositionalEncodingMM(\n",
    "            d_model=d_model\n",
    "        )\n",
    "        self.encoder = Encoder(d_model=d_model, n_heads=attention_heads, num_layers=n_encoder_layers)\n",
    "\n",
    "        self.query_refiner = QueryRefiner(d_model=d_model, num_heads=attention_heads, num_queries=n_queries)\n",
    "        self.decoder = HyperspectralDecoder(d_model=d_model, n_heads=attention_heads, num_layers=n_decoder_layers)\n",
    "\n",
    "\n",
    "        self.head = None\n",
    "        match model_type:\n",
    "            case ModelType.CLASSIFICATION:\n",
    "                self.head = ClassifierPredictor(\n",
    "                    num_classes=2,\n",
    "                    embedding_dim=d_model,\n",
    "                )\n",
    "            case ModelType.SEGMENTATION:\n",
    "                self.head = BoxAndMaskPredictor(\n",
    "                    result_width=image_width,\n",
    "                    result_height=image_height,\n",
    "                    fpn_channels=backbone_out_channels,\n",
    "                    embedding_dim=d_model,\n",
    "                )\n",
    "            case ModelType.ONLY_BBOX:\n",
    "                self.head = BBoxPrediction(d_model=d_model)\n",
    "\n",
    "    def forward(self, image: torch.Tensor, filtered_image: torch.Tensor) -> tuple[\n",
    "        torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        TODO docs, tests\n",
    "        \"\"\"\n",
    "        # get image size\n",
    "        batch_size, channels, height, width = image.shape\n",
    "\n",
    "        f_comb_proj, f_comb = self.backbone(image)\n",
    "\n",
    "        positional_encoding = self.positional_encoding(f_comb)[0].expand(batch_size, -1, -1, -1)\n",
    "\n",
    "        f_mc = self.spectral_feature_generator(filtered_image)\n",
    "        f_mc = f_mc.permute(0, 2, 3, 1)\n",
    "\n",
    "        q_ref = self.query_refiner(f_mc)\n",
    "        f_e = self.encoder((f_comb_proj + positional_encoding).flatten(2).permute(0, 2, 1))\n",
    "\n",
    "\n",
    "        e_out = self.decoder(\n",
    "            (f_e.permute(0, 2, 1).view(batch_size, -1, int(height / 32), int(width / 32)) + positional_encoding).flatten(2).permute(0, 2, 1),\n",
    "            q_ref\n",
    "        )\n",
    "\n",
    "        result = self.head(e_out, f_e)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35324cba37646987",
   "metadata": {},
   "source": [
    "### Prepare models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fbb9f2106cd78f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T00:39:31.879866Z",
     "start_time": "2024-12-17T00:39:31.876766Z"
    }
   },
   "outputs": [],
   "source": [
    "def setup_model(model: nn.Module, lr: float, device: str):\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()  # Binary classification\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef59708b5382f94",
   "metadata": {},
   "source": [
    "## Prepare training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac3224cd7ff8f852",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T00:39:31.888328Z",
     "start_time": "2024-12-17T00:39:31.884824Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_progress_bar(percentage, loss):\n",
    "    bar_length = 50  # Length of the progress bar\n",
    "    filled_length = int(bar_length * percentage // 100)\n",
    "    bar = '=' * filled_length + '-' * (bar_length - filled_length)\n",
    "    sys.stdout.write(f\"\\r[{bar}] {percentage:.2f}% [Loss: {loss:.6f}]\")\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4c4eb62cccb104c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T00:39:31.896932Z",
     "start_time": "2024-12-17T00:39:31.893628Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(criterion, device, epochs, model, optimizer, dataloader, model_handler,  log_batches: bool = False):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):  # Adjust the number of epochs\n",
    "        running_loss = 0.0\n",
    "        for batch_id, (images, mag1c, filtered_image, labels) in enumerate(dataloader):  # Assume a PyTorch DataLoader is set up\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_image = torch.cat((images, mag1c), dim=1).to(device)\n",
    "            filtered_image = filtered_image.to(device)\n",
    "            labels = labels.long().to(device)\n",
    "\n",
    "            outputs = model(input_image, filtered_image)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if log_batches and (batch_id + 1) % 10 == 0:\n",
    "                print_progress_bar(batch_id / len(dataloader) * 100, running_loss / (batch_id + 1))\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(dataloader)}\")\n",
    "        model_handler.save_raw_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead9dd1c2d9c4483",
   "metadata": {},
   "source": [
    "## Prepare evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deed82ce61ddca3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T00:39:31.909217Z",
     "start_time": "2024-12-17T00:39:31.905232Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(criterion, device, model, dataloader, measurer):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_id, (images, mag1c, filtered_image, labels) in enumerate(dataloader):\n",
    "        input_image = torch.cat((images, mag1c), dim=1).to(device)\n",
    "        filtered_image = filtered_image.to(device)\n",
    "        labels = labels.long().to(device)\n",
    "\n",
    "        outputs = model(input_image, filtered_image)\n",
    "\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        all_predictions.append(predictions.cpu().detach())\n",
    "        all_labels.append(labels.cpu().detach())\n",
    "\n",
    "    measures = measurer.compute_measures(torch.cat(all_predictions), torch.cat(all_labels))\n",
    "    print(f\"Validation loss: {running_loss / len(dataloader)}.\\nMeasures:\\n{measures}\")\n",
    "    return measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5f24876565680a",
   "metadata": {},
   "source": [
    "# Train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72d2ee60759244d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:52:12.995661Z",
     "start_time": "2024-12-17T00:39:31.912221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 99.07% [Loss: 0.591541]Epoch 1, Loss: 0.5915861721607001\n",
      "[=================================================-] 99.07% [Loss: 0.411100]Epoch 2, Loss: 0.4120999602111906\n",
      "[=================================================-] 99.07% [Loss: 0.395228]Epoch 3, Loss: 0.3958871129600281\n",
      "[=================================================-] 99.07% [Loss: 0.364992]Epoch 4, Loss: 0.36609527144891885\n",
      "[=================================================-] 99.07% [Loss: 0.373850]Epoch 5, Loss: 0.3743855593945557\n",
      "[=================================================-] 99.07% [Loss: 0.408641]Epoch 6, Loss: 0.40920289716375785\n",
      "[=================================================-] 99.07% [Loss: 0.383305]Epoch 7, Loss: 0.3821487782855574\n",
      "[=================================================-] 99.07% [Loss: 0.372066]Epoch 8, Loss: 0.37252905439311595\n",
      "[=================================================-] 99.07% [Loss: 0.317049]Epoch 9, Loss: 0.3159587417165189\n",
      "[=================================================-] 99.07% [Loss: 0.272355]Epoch 10, Loss: 0.2725488304827784\n",
      "[=================================================-] 99.07% [Loss: 0.230669]Epoch 11, Loss: 0.2319495763293877\n",
      "[=================================================-] 99.07% [Loss: 0.193835]Epoch 12, Loss: 0.1934694755717008\n",
      "[=================================================-] 99.07% [Loss: 0.157554]Epoch 13, Loss: 0.15829558904394003\n",
      "[=================================================-] 99.07% [Loss: 0.154863]Epoch 14, Loss: 0.15558023713330782\n",
      "[=================================================-] 99.07% [Loss: 0.149161]Epoch 15, Loss: 0.14917698699577256\n",
      "Validation loss: 0.17559683605843382.\n",
      "Measures:\n",
      "         TP        FP        FN        TN  Precision  Sensitivity  \\\n",
      "0  0.461988  0.038012  0.026316  0.473684   0.923975     0.946106   \n",
      "\n",
      "   Specificity       NPV       FPR  Accuracy   F-Score       IoU      MCC  \\\n",
      "0     0.925712  0.947367  0.074286  0.935672  0.934909  0.877778  0.87158   \n",
      "\n",
      "        AUC       CI  \n",
      "0  0.935911  0.05307  \n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 1e-4\n",
    "\n",
    "from models.Transformer.MethaneMapper.model import TransformerModel\n",
    "\n",
    "train_dataloader, test_dataloader = setup_dataloaders(\n",
    "    batch_size=4,\n",
    "    image_info_class=MMClassifierDatasetInfo,\n",
    "    crop_size=1,\n",
    "    train_type=DatasetType.TRAIN,\n",
    ")\n",
    "model = TransformerModel(\n",
    "    n_queries=5,\n",
    "    n_decoder_layers=5,\n",
    "    n_encoder_layers=5,\n",
    "    d_model=256,\n",
    ")\n",
    "model, criterion, optimizer = setup_model(model, lr, device)\n",
    "model_handler = ModelFilesHandler()\n",
    "measurer = MeasureToolFactory.get_measure_tool(MeasuresModelType.TRANSFORMER)\n",
    "\n",
    "train(criterion, device, epochs, model, optimizer, train_dataloader, model_handler, log_batches=True)\n",
    "measures = evaluate(criterion, device, model, test_dataloader, measurer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519706b2f962064",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1aae91ca9193b16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:52:13.571674Z",
     "start_time": "2024-12-17T01:52:13.068728Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trained_models\\\\model_transformer_classifier_2024_12_17_02_52_13.pickle'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_handler.save_model(\n",
    "    model=model,\n",
    "    metrics = measures,\n",
    "    model_type=MeasuresModelType.TRANSFORMER_CLASSIFIER,\n",
    "    epoch=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73849f1-ee07-49e7-b331-576bdd71b00b",
   "metadata": {},
   "source": [
    "## Restore environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f9ffea-dbd4-47e9-9c4e-56a3af5f01e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\".\\notebooks\\models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
