{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Eniviroment setup",
   "id": "bd2788c92659ad2f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from typing import Optional\n",
    "\n",
    "from transformers import DetrConfig, DetrForObjectDetection\n",
    "\n",
    "from dataset.dataset_info import ClassifierDatasetInfo\n",
    "from dataset.dataset_type import DatasetType\n",
    "from dataset.STARCOP_dataset import STARCOPDataset\n",
    "\n",
    "from models.Tools.FilesHandler.model_files_handler import ModelFilesHandler\n",
    "from models.Tools.Measures.measure_tool_factory import MeasureToolFactory\n",
    "from models.Tools.Measures.model_type import ModelType"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup datasets\n",
    "STARCOPDataset is custom class that derives torch.utils.data.Dataset class. It's defined in dataset module.  "
   ],
   "id": "f1080e782a0be239"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def setup_dataloaders(data_path: str = r\"data\", batch_size: int = 32, train_type = DatasetType.EASY_TRAIN):\n",
    "    train_dataset = STARCOPDataset(\n",
    "        data_path=data_path,\n",
    "        data_type=train_type,\n",
    "        image_info_class=ClassifierDatasetInfo,\n",
    "    )\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = STARCOPDataset(\n",
    "        data_path=data_path,\n",
    "        data_type=DatasetType.TEST,\n",
    "        image_info_class=ClassifierDatasetInfo,\n",
    "    )\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ],
   "id": "633a22ebce25443b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup models\n",
    "\n",
    "### Model class"
   ],
   "id": "55aade7bf5249f30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CustomDetrForClassification(nn.Module):\n",
    "    def __init__(self, detr_model_name=\"facebook/detr-resnet-50\", num_channels=9, num_classes=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load pre-trained DETR model\n",
    "        config = DetrConfig.from_pretrained(detr_model_name)\n",
    "        config.num_labels = num_classes  # Number of classification labels\n",
    "        config.use_decoder = True  # Ensure the decoder is retained for processing queries\n",
    "        config.output_hidden_states = True  # Ensure hidden states are returned\n",
    "        self.detr = DetrForObjectDetection(config=config)\n",
    "\n",
    "        # Modify the first convolutional layer of the backbone to accept 9 channels\n",
    "        backbone = self.detr.model.backbone\n",
    "        conv1 = backbone.conv_encoder.model.conv1\n",
    "        new_conv1 = nn.Conv2d(\n",
    "            in_channels=num_channels,\n",
    "            out_channels=conv1.out_channels,\n",
    "            kernel_size=conv1.kernel_size,\n",
    "            stride=conv1.stride,\n",
    "            padding=conv1.padding,\n",
    "            bias=conv1.bias,\n",
    "        )\n",
    "\n",
    "        # Replace the original conv1 with the new one\n",
    "        backbone.conv_encoder.model.conv1 = new_conv1\n",
    "\n",
    "        # Freeze backbone layers except the first conv layer\n",
    "        for name, param in backbone.named_parameters():\n",
    "            if \"conv1\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Add a classification head to process the outputs of the decoder\n",
    "        hidden_size = config.d_model\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        # Pass inputs through DETR backbone and transformer\n",
    "        outputs = self.detr.model(pixel_values)\n",
    "\n",
    "        # Extract decoder output (shape: batch_size, num_queries, d_model)\n",
    "        decoder_output = outputs.decoder_hidden_states[-1]\n",
    "\n",
    "        # Apply classification head (average over all queries)\n",
    "        logits = self.classifier(decoder_output.mean(dim=1))  # (batch_size, num_classes)\n",
    "\n",
    "        return logits"
   ],
   "id": "8f28a27507f2ad99"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prepare models ",
   "id": "35324cba37646987"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def setup_model(model: nn.Module, lr: float, device: str):\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()  # Binary classification\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    return model, criterion, optimizer"
   ],
   "id": "4fbb9f2106cd78f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prepare training function",
   "id": "ef59708b5382f94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train(criterion, device, epochs, model, optimizer, dataloader, transform: Optional[transforms] = None, log_batches: bool = False):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):  # Adjust the number of epochs\n",
    "        running_loss = 0.0\n",
    "        for batch_id, (images, mag1c, labels) in enumerate(dataloader):  # Assume a PyTorch DataLoader is set up\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_image = torch.cat((images, mag1c), dim=1)\n",
    "            labels = labels.long().to(device)\n",
    "\n",
    "            outputs = model((transform(input_image) if transform else  input_image).to(device))\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if log_batches and (batch_id + 1) % 10 == 0:\n",
    "                print(f\"Batch: {batch_id + 1}, Loss: {running_loss / (batch_id + 1)}\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(dataloader)}\")"
   ],
   "id": "b4c4eb62cccb104c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prepare evaluate function",
   "id": "ead9dd1c2d9c4483"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate(criterion, device, model, dataloader, measurer, transform: Optional[transforms] = None):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_id, (images, mag1c, labels) in enumerate(dataloader):\n",
    "        input_image = torch.cat((images, mag1c), dim=1)\n",
    "        labels = labels.long().to(device)\n",
    "\n",
    "        outputs = model((transform(input_image) if transform else  input_image).to(device))\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        all_predictions.append(predictions.cpu().detach())\n",
    "        all_labels.append(labels.cpu().detach())\n",
    "\n",
    "    measures = measurer.compute_measures(torch.cat(all_predictions), torch.cat(all_labels))\n",
    "    print(f\"Validation loss: {running_loss / len(dataloader)}.\\nMeasures:\\n{measures}\")\n",
    "    return measures"
   ],
   "id": "deed82ce61ddca3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train model ",
   "id": "5a5f24876565680a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "epochs = 40\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 1e-5\n",
    "\n",
    "train_dataloader, test_dataloader = setup_dataloaders(batch_size=16)\n",
    "model = CustomDetrForClassification()\n",
    "model, criterion, optimizer = setup_model(model, lr, device)\n",
    "model_handler = ModelFilesHandler()\n",
    "measurer = MeasureToolFactory.get_measure_tool(ModelType.TRANSFORMER)\n",
    "\n",
    "train(criterion, device, epochs, model, optimizer, train_dataloader, log_batches = True)\n",
    "measures = evaluate(criterion, device, model, test_dataloader, measurer)\n"
   ],
   "id": "72d2ee60759244d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save model",
   "id": "e519706b2f962064"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_handler.save_model(\n",
    "    model=model,\n",
    "    metrics = measures,\n",
    "    model_type=ModelType.TRANSFORMER_CLASSIFIER,\n",
    "    epoch=epochs,\n",
    ")"
   ],
   "id": "d1aae91ca9193b16"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
